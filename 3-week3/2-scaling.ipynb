{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data has been loaded and some initial cleaning is done, it is often necessary to perform several addition rounds of data preparation.  This can include scaling data, removing outliers, re-encoding variables, and imputing missing data.  We'll cover these aspects below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Many machine learning algorithms, such as k-means clustering and support vector machines, use distance metrics like Euclidean distance to compare points in the feature space. Features with large numeric ranges can dominate the distance computation, thereby affecting the algorithm's performance. Similarly, optimization algorithms like gradient descent converge more quickly when features are on similar scales.\n",
    "\n",
    "### Working Example - Impact of Scaling on K-Means Clustering\n",
    "\n",
    "Let's consider a simple synthetic dataset with two features `X1` and `X2`, where `X1` has values ranging between 0 and 10, but are separated into two groups along the axis. `X2` ranges from 0 to 1000, and so dominates the distance function. We'll cluster the data using k-means before and after scaling to see the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X1a = np.random.uniform(0, 4, 50)\n",
    "X1b = np.random.uniform(6, 10, 50)\n",
    "X1 = np.concatenate([X1a,X1b])\n",
    "X2 = np.random.uniform(0, 1000, 100)\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Cluster without scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering without Scaling')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cluster after scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering with Scaling')\n",
    "plt.xlabel('X1 (scaled)')\n",
    "plt.ylabel('X2 (scaled)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to scale\n",
    "\n",
    "Not all distributions are created equal!  It is important to examine your distributions before scaling parameters, or else your scaling efforts might not yield any improvements.\n",
    "\n",
    "Two common types of distributions are normal and power-law (heavy-tailed) distributions.  These are easy to recognize by plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "\n",
    "# Power-law (exponential) distribution\n",
    "power_law_data = np.random.exponential(scale=10, size=n_samples)\n",
    "\n",
    "# Scaled\n",
    "scaled_power_law = np.log1p(power_law_data)\n",
    "\n",
    "# Normal distribution\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "normal_data_2D = normal_data.reshape(-1, 1)\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_normal_data_2D = scaler.fit_transform(normal_data_2D)\n",
    "\n",
    "# Convert back to 1D array\n",
    "scaled_normal_data = scaled_normal_data_2D.ravel()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot power-law distribution\n",
    "axes[0,0].hist(power_law_data, bins=30, color='blue', edgecolor='black')\n",
    "axes[0,0].set_title(\"Power-law (Exponential) Distribution\")\n",
    "axes[0,0].set_xlabel(\"Value\")\n",
    "axes[0,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled power-law distribution\n",
    "axes[1,0].hist(scaled_power_law, bins=30, color='blue', edgecolor='black')\n",
    "axes[1,0].set_title(\"Scaled Exponential Distribution\")\n",
    "axes[1,0].set_xlabel(\"Value\")\n",
    "axes[1,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot normal distribution\n",
    "axes[0,1].hist(normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[0,1].set_title(\"Normal Distribution\")\n",
    "axes[0,1].set_xlabel(\"Value\")\n",
    "axes[0,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled normal distribution\n",
    "axes[1,1].hist(scaled_normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[1,1].set_title(\"Normal Distribution\")\n",
    "axes[1,1].set_xlabel(\"Value\")\n",
    "axes[1,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Z-scaling**: Use it when the feature roughly follows a normal distribution or when you don't have information about the distribution. It transforms the data into a distribution with a mean of 0 and a standard deviation of 1.\n",
    "  \n",
    "- **Log-Scaling**: It is useful for features that follow a power-law distribution. In these cases, log-scaling can help equalize the ranges and variances across features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the following, I've created a sample dataset with an exponential feature and a normal feature. Try using the different scaling methods before running the classifier.  How do your results change:\n",
    "\n",
    "1.  If you scale the exponential feature using a StandardScaler\n",
    "2.  If you scale the exponential feature using a Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n_samples = 1000\n",
    "\n",
    "# Feature 1: Power law (exponential) distribution\n",
    "X1 = np.random.exponential(scale=5, size=n_samples)\n",
    "\n",
    "# Feature 2: Normal distribution\n",
    "X2 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "\n",
    "# Create labels: simple linear relation to X1 and X2\n",
    "y = np.array([1 if x1 + 0.001 * x2 > 1 else 0 for x1, x2 in zip(X1, X2)])\n",
    "flip_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "y[flip_indices] = 1 - y[flip_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Combine features into single data array\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression without scaling\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression without Scaling - Test Accuracy: {lr.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# Plot original features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Feature 1 (Power law)')\n",
    "plt.ylabel('Feature 2 (Normal)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
